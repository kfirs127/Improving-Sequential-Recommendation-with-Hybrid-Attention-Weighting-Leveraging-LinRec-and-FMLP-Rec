{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTea0NZwoKpi"
      },
      "source": [
        "## FMLP-Rec and LinRec Training and Data Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7UZ7Pm3oC1P"
      },
      "source": [
        "### FMLP-Rec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-L-QG0M6n5S"
      },
      "source": [
        "#### Preprocess ML-1M Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIW9PYou6L2h",
        "outputId": "457aed9f-9101-4081-e14f-2b9c685cf909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FMLP-Rec'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 20 (delta 0), reused 20 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (20/20), 8.41 MiB | 18.02 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/RUCAIBox/FMLP-Rec.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TNDSOzEo6RNA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from collections import Counter\n",
        "import csv\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN-WmbR66TGR",
        "outputId": "ea624b8f-18c7-47ec-89de-1419b446d25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGMuJGg4BIzc"
      },
      "source": [
        "#### Preprocess datasets and save mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iTlsRmYR6U41"
      },
      "outputs": [],
      "source": [
        "data_file_ml1m = '/content/drive/MyDrive/ml-1m.zip'\n",
        "extract_path_ml1m = 'ml-1m'\n",
        "ratings_file_ml1m = 'ml-1m/ratings.dat'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PuPSy-ok6WWJ"
      },
      "outputs": [],
      "source": [
        "processed_file_ml1m = \"ML-1M.txt\"\n",
        "sample_file_ml1m = \"ML-1M_sample.txt\"\n",
        "destination_folder = \"FMLP-Rec/data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s2BhjZD56X43"
      },
      "outputs": [],
      "source": [
        "def convert_zip_to_df(zip_path, extract_path, ratings_file):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    ml1m_path = os.path.join(extract_path, ratings_file)\n",
        "    df = pd.read_csv(ml1m_path, sep=\"::\", engine=\"python\", names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qyaLJahM6c6O"
      },
      "outputs": [],
      "source": [
        "def filter_k_core(df, k=5):\n",
        "    while True:\n",
        "        user_counts = df[\"user_id\"].value_counts()\n",
        "        item_counts = df[\"item_id\"].value_counts()\n",
        "\n",
        "        df = df[df[\"user_id\"].isin(user_counts[user_counts >= k].index)]\n",
        "        df = df[df[\"item_id\"].isin(item_counts[item_counts >= k].index)]\n",
        "\n",
        "        new_user_counts = df[\"user_id\"].value_counts()\n",
        "        new_item_counts = df[\"item_id\"].value_counts()\n",
        "\n",
        "        if len(new_user_counts) == len(user_counts) and len(new_item_counts) == len(item_counts):\n",
        "            break\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "h7jbRR_6H_5I"
      },
      "outputs": [],
      "source": [
        "def save_mappings_to_csv(user_map, item_map, dataset):\n",
        "\n",
        "    # Create DataFrames from the mappings\n",
        "    df_user = pd.DataFrame(list(user_map.items()), columns=['old_user_id', 'new_user_id'])\n",
        "    df_item = pd.DataFrame(list(item_map.items()), columns=['old_item_id', 'new_item_id'])\n",
        "\n",
        "    # Save to CSV files\n",
        "    df_user.to_csv(f'user_map_{dataset}.csv', index=False)\n",
        "    df_item.to_csv(f'item_map_{dataset}.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ofon32YZ6fDF"
      },
      "outputs": [],
      "source": [
        "def run_preprocess(data_file, extract_path, ratings_file, sample_file, processed_file, kcore=False, min_rating=0):\n",
        "\n",
        "    data = convert_zip_to_df(data_file, extract_path, ratings_file)\n",
        "\n",
        "    # Keep only interactions where rating > min_rating\n",
        "    data = data[data[\"rating\"] > min_rating]\n",
        "\n",
        "    # apply k-core filtering (ensuring each user and item has at least k interactions)\n",
        "    if kcore:\n",
        "      data = filter_k_core(data, k=5)\n",
        "\n",
        "    user_map = {old: new+1 for new, old in enumerate(data[\"user_id\"].unique())}\n",
        "    item_map = {old: new+1 for new, old in enumerate(data[\"item_id\"].unique())}\n",
        "\n",
        "    save_mappings_to_csv(user_map, item_map, extract_path)\n",
        "\n",
        "    data[\"user_id\"] = data[\"user_id\"].map(user_map)\n",
        "    data[\"item_id\"] = data[\"item_id\"].map(item_map)\n",
        "\n",
        "    # group by user_id, sort by timestamp, and aggregate item interactions\n",
        "    data = data.sort_values(by=[\"user_id\", \"timestamp\"])\n",
        "    grouped = data.groupby(\"user_id\")[\"item_id\"].apply(list).to_dict()\n",
        "\n",
        "    grouped_data = data.groupby(\"user_id\")[\"item_id\"].apply(lambda x: \" \".join(map(str, x))).reset_index()\n",
        "    grouped_data[\"formatted\"] = grouped_data[\"user_id\"].astype(str) + \" \" + grouped_data[\"item_id\"]\n",
        "    grouped_data[\"formatted\"].to_csv(f\"{processed_file}\", index=False, header=False)\n",
        "\n",
        "    print(f\"Processing complete. Saved as {processed_file}\")\n",
        "\n",
        "    # 99 negative samples per user required\n",
        "    all_items = set(data[\"item_id\"].unique())\n",
        "\n",
        "    with open(sample_file, \"w\") as f:\n",
        "        for user_id, pos_items in grouped.items():\n",
        "            positive_items = set(pos_items) if isinstance(pos_items, (set, list, pd.Series)) else {pos_items}\n",
        "            negative_samples = list(all_items - positive_items)\n",
        "            sampled_negatives = np.random.choice(negative_samples, 99, replace=False) if len(negative_samples) >= 99 else negative_samples\n",
        "            f.write(f\"{user_id} \" + \" \".join(map(str, sampled_negatives)) + \"\\n\")\n",
        "\n",
        "    print(f\"Sample file '{sample_file}' created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eib3JaiCJQxw"
      },
      "source": [
        "Preprocess ML-1M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjniEaSC6hq0",
        "outputId": "70ca71f0-26bc-4e38-fabe-0b2e15400ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete. Saved as ML-1M.txt\n",
            "Sample file 'ML-1M_sample.txt' created successfully.\n"
          ]
        }
      ],
      "source": [
        "run_preprocess(data_file_ml1m, extract_path_ml1m, ratings_file_ml1m, sample_file_ml1m, processed_file_ml1m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gaLSDKP6kGM",
        "outputId": "69fb43ca-dc1b-42fc-d35c-343dcf754a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files moved to FMLP-Rec/data/\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "shutil.move(processed_file_ml1m, os.path.join(destination_folder, processed_file_ml1m))\n",
        "shutil.move(sample_file_ml1m, os.path.join(destination_folder, sample_file_ml1m))\n",
        "\n",
        "print(f\"Files moved to {destination_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwIhHTr5JjYn"
      },
      "source": [
        "Preprocess Beauty"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vda93Je1HjC9",
        "outputId": "912dc23a-e2cc-4c99-e750-4c95e42ae951"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i4JmP1hpKJRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c54474c-7ed7-4c31-a3a5-311c25117f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CIKM2020-S3Rec'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 128 (delta 15), reused 10 (delta 10), pack-reused 103 (from 1)\u001b[K\n",
            "Receiving objects: 100% (128/128), 60.87 MiB | 22.17 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/RUCAIBox/CIKM2020-S3Rec.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_P0Pk0HtL4K_"
      },
      "outputs": [],
      "source": [
        "# change the path to the dataset\n",
        "\n",
        "!sed -i 's|/path/reviews_|/content/drive/MyDrive/reviews_|g' /content/CIKM2020-S3Rec/data/data_process.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j6TVho2qMkps"
      },
      "outputs": [],
      "source": [
        "# remove the runs to preprocess unecessary data\n",
        "\n",
        "!sed -i '/^amazon_datas =/,$d' /content/CIKM2020-S3Rec/data/data_process.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save user/item id mappings\n",
        "\n",
        "!sed -i $'/data_maps = id_map(user_items)/a\\\\\n",
        "   import json\\\\n\\\n",
        "   with open(\"/content/CIKM2020-S3Rec/data/Beauty_id_maps.json\", \"w\") as f:\\\\n\\\n",
        "       json.dump(data_maps, f)\\\\n\\\n",
        "   print(\"Mapping saved to data/Beauty_id_maps.json\")' /content/CIKM2020-S3Rec/data/data_process.py"
      ],
      "metadata": {
        "id": "R0WAvrj1J-iO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run preprocessing on the Amazon Beauty dataset and save the user/item id mappings\n",
        "\n",
        "%%bash\n",
        "cat << 'END' >> /content/CIKM2020-S3Rec/data/data_process.py\n",
        "\n",
        "main('Beauty', data_type='Amazon')\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open('/content/CIKM2020-S3Rec/data/Beauty_id_maps.json', 'r') as f:\n",
        "    data_maps = json.load(f)\n",
        "df_user = pd.DataFrame(list(data_maps['id2user'].items()), columns=['new_user_id', 'original_user_id'])\n",
        "df_item = pd.DataFrame(list(data_maps['id2item'].items()), columns=['new_item_id', 'original_item_id'])\n",
        "\n",
        "import os\n",
        "os.makedirs('./output', exist_ok=True)\n",
        "df_user.to_csv(os.path.join('./output', 'id2user.csv'), index=False)\n",
        "df_item.to_csv(os.path.join('./output', 'id2item.csv'), index=False)\n",
        "print(\"Mapping files saved to ./output\")\n",
        "END"
      ],
      "metadata": {
        "id": "1fQAzwwIGc7o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/CIKM2020-S3Rec/data/ && python data_process.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5POAFKg6D7Ys",
        "outputId": "62f534e5-9341-4779-862d-5d9a72107498"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beauty Raw data has been processed! Lower than 0.0 are deleted!\n",
            "User 5-core complete! Item 5-core complete!\n",
            "Mapping saved to data/Beauty_id_maps.json\n",
            "Total User: 22363, Avg User: 8.8764, Min Len: 5, Max Len: 204\n",
            "Total Item: 12101, Avg Item: 16.4038, Min Inter: 5, Max Inter: 431\n",
            "Iteraction Num: 198502, Sparsity: 99.93%\n",
            "Mapping files saved to ./output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYTicShVn5ze"
      },
      "source": [
        "#### Create CSVs for item predictions (validation and test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "L1xLyKeQAlQo"
      },
      "outputs": [],
      "source": [
        "# save internal user ids and item recommendations\n",
        "\n",
        "!sed -i $'/self\\.model\\.eval()/a\\\\\n",
        "           all_user_ids = []\\\\n\\\\\n",
        "           all_candidate_ids = []'  /content/FMLP-Rec/trainers.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SgUl0u36YWcR"
      },
      "outputs": [],
      "source": [
        "!sed -i '/return self.get_full_sort_score(epoch, answer_list, pred_list)/,/^return self\\.get_sample_scores(epoch, pred_list)/ s/^\\(.*for i, batch in rec_data_iter:\\)/                answer_list = None\\n\\1/' /content/FMLP-Rec/trainers.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XuhHXeU0YpUG"
      },
      "outputs": [],
      "source": [
        "!sed -i $'/pred_list = test_logits/a\\\\\n",
        "                       answer_list = answers.cpu().data.numpy()'  /content/FMLP-Rec/trainers.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FU3W23_zYwIA"
      },
      "outputs": [],
      "source": [
        "!sed -i $'/pred_list = np.append(pred_list, test_logits, axis=0)/a\\\\\n",
        "                       answer_list = np.append(answer_list, answers.cpu().data.numpy(), axis=0)'  /content/FMLP-Rec/trainers.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i $'/test_neg_items = torch.cat((answers.unsqueeze(-1), sample_negs), -1)/a\\\\\n",
        "                    all_candidate_ids.append(test_neg_items.cpu().numpy())'  /content/FMLP-Rec/trainers.py"
      ],
      "metadata": {
        "id": "CPo0IMIlNdQy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4-DY62ad2PoP"
      },
      "outputs": [],
      "source": [
        "# save outputs of user id, recommendations, and ground truth\n",
        "\n",
        "!sed -i $'/self.get_sample_scores(epoch, pred_list)/i\\\\\n",
        "               all_user_ids = np.concatenate(all_user_ids, axis=0).flatten()\\\\n\\\\\n",
        "               all_candidate_ids = np.concatenate(all_candidate_ids, axis=0)\\\\n\\\\\n",
        "               import pandas as pd\\\\n\\\\\n",
        "               import os\\\\n\\\\\n",
        "               sorted_idx = np.argsort(-pred_list, axis=1)\\\\n\\\\\n",
        "               top_k = sorted_idx[:, :20]\\\\n\\\\\n",
        "               top_k_item_ids = np.take_along_axis(all_candidate_ids, top_k, axis=1)\\\\n\\\\\n",
        "               df = pd.DataFrame({\\\\n\\\\\n",
        "                   \"user_id\": all_user_ids,\\\\n\\\\\n",
        "                   \"predicted_top20\": [list(row) for row in top_k_item_ids],\\\\n\\\\\n",
        "                   \"ground_truth\": answer_list.tolist()\\\\n\\\\\n",
        "               })\\\\n\\\\\n",
        "               if dataloader == self.eval_dataloader:\\\\n\\\\\n",
        "                   csv_path = os.path.join(self.args.output_dir, \"valid_predictions.csv\")\\\\n\\\\\n",
        "                   df.to_csv(csv_path, index=False)\\\\n\\\\\n",
        "               elif dataloader == self.test_dataloader:\\\\n\\\\\n",
        "                   csv_path = os.path.join(self.args.output_dir, \"test_predictions.csv\")\\\\n\\\\\n",
        "                   df.to_csv(csv_path, index=False)' /content/FMLP-Rec/trainers.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mkCgSyzpBjFE"
      },
      "outputs": [],
      "source": [
        "!sed -i $'/user_ids, input_ids, answers, _, neg_answer = batch/ a\\\\\n",
        "                   all_user_ids.append(user_ids.cpu().numpy())' /content/FMLP-Rec/trainers.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pU7whq8WCWaN"
      },
      "outputs": [],
      "source": [
        "!sed -i $'/user_ids, input_ids, answers, _, sample_negs = batch/ a\\\\\n",
        "                   all_user_ids.append(user_ids.cpu().numpy())' /content/FMLP-Rec/trainers.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Jj5RBdVdAb"
      },
      "source": [
        "#### Run FMLP-Rec on Amazon Beauty Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X30uWNZyViOF"
      },
      "outputs": [],
      "source": [
        "!cd /content/FMLP-Rec/ && python main.py --data_name='Beauty'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMRMlAhvVjhJ"
      },
      "source": [
        "#### Run FMLP-Rec on ML-1M Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PrYICU7uVpbS"
      },
      "outputs": [],
      "source": [
        "# add in ML-1M dataset\n",
        "\n",
        "!sed -i \"s/^sequential_data_list = \\['Beauty','Sports_and_Outdoors','Toys_and_Games','Yelp'\\]/sequential_data_list = \\['Beauty','Sports_and_Outdoors','Toys_and_Games','Yelp','ML-1M'\\]/\" /content/FMLP-Rec/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLZ1JHUCVvSm"
      },
      "outputs": [],
      "source": [
        "!cd /content/FMLP-Rec/ && python main.py --data_name='ML-1M'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifHFr2bW_ZxD"
      },
      "source": [
        "### LinRec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jqr8Oun_bIH"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install recbole\n",
        "!pip install ray\n",
        "!pip install kmeans-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r_U8dym_es_"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3avPXxp0_hHM",
        "outputId": "94171743-3602-4d47-9b6d-038a0627eddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LinRec'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 38 (delta 18), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (38/38), 23.09 KiB | 11.54 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Applied-Machine-Learning-Lab/LinRec.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmbCtX5GZCU-"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/LinRec/output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmCTJfMg_Jyk"
      },
      "outputs": [],
      "source": [
        "# create a mapping of the internal ids for user and item to the originals\n",
        "\n",
        "!sed -i $'/create_dataset(config)/a\\\\\n",
        "   print(\"User mapping (internal -> original):\", dataset.id2token(\"user_id\", list(range(dataset.num(\"user_id\")))))\\\\n\\\n",
        "   print(\"Item mapping (internal -> original):\", dataset.id2token(\"item_id\", list(range(dataset.num(\"item_id\")))))\\\\n\\\n",
        "\\\\n\\\n",
        "   import pandas as pd\\\\n\\\n",
        "   user_ids = list(range(dataset.num(\"user_id\")))\\\\n\\\n",
        "   user_tokens = dataset.id2token(\"user_id\", user_ids)\\\\n\\\n",
        "   user_mapping = {internal: token for internal, token in zip(user_ids, user_tokens)}\\\\n\\\n",
        "\\\\n\\\n",
        "   item_ids = list(range(dataset.num(\"item_id\")))\\\\n\\\n",
        "   item_tokens = dataset.id2token(\"item_id\", item_ids)\\\\n\\\n",
        "   item_mapping = {internal: token for internal, token in zip(item_ids, item_tokens)}\\\\n\\\n",
        "\\\\n\\\n",
        "   user_df = pd.DataFrame(list(user_mapping.items()), columns=[\"internal_user_id\", \"original_user_id\"])\\\\n\\\n",
        "   item_df = pd.DataFrame(list(item_mapping.items()), columns=[\"internal_item_id\", \"original_item_id\"])\\\\n\\\n",
        "   user_df.to_csv(config[\"output_dir\"] + \"/id2user.csv\", index=False)\\\\n\\\n",
        "   item_df.to_csv(config[\"output_dir\"] + \"/id2item.csv\", index=False)' /usr/local/lib/python3.11/dist-packages/recbole/quick_start/quick_start.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-PDhRDVyeyX"
      },
      "outputs": [],
      "source": [
        "# add new metric that saves the user ids and their top k recommendations\n",
        "\n",
        "%%bash\n",
        "cat << 'END' >> /usr/local/lib/python3.11/dist-packages/recbole/evaluator/metrics.py\n",
        "\n",
        "from recbole.evaluator.base_metric import AbstractMetric\n",
        "from recbole.utils import EvaluatorType\n",
        "class RecItems(AbstractMetric):\n",
        "    \"\"\"A dummy metric that forces the collector to gather the full predicted item list.\"\"\"\n",
        "    metric_type = EvaluatorType.RANKING\n",
        "    metric_need = [\"rec.items\", \"data.label\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.requirements = [\"rec.items\", \"data.label\"]\n",
        "\n",
        "    def calculate_metric(self, dataobject):\n",
        "        try:\n",
        "            rec_items = dataobject.get(\"rec.items\")\n",
        "        except Exception as e:\n",
        "            return {}\n",
        "        if rec_items.size(1) < 20:\n",
        "            top20 = rec_items\n",
        "        else:\n",
        "            top20 = rec_items[:, :20]\n",
        "        top20_list = top20.cpu().numpy().tolist()\n",
        "        user_ids = list(range(len(top20_list)))\n",
        "        return {\"user_id\": user_ids, \"recitems@20\": top20_list}\n",
        "END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxpaMEqVkgyh"
      },
      "outputs": [],
      "source": [
        "!sed -i 's/\\(\"Precision\"\\)[[:space:]]*\\]/\\1,\"RecItems\"]/' /content/LinRec/overall.yaml\n",
        "!sed -i \"s/\\('metrics': \\[[^]]*\\)\\(]\\)/\\1, 'RecItems'\\2/\" /content/LinRec/run.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX2TLpcpMJOg"
      },
      "outputs": [],
      "source": [
        "# save mapping between user id and label for both validation and test sets\n",
        "\n",
        "!sed -i $'/^        return next_ds/ i\\\\\n",
        "       valid_indices = next_index[1]\\\\n\\\n",
        "       test_indices = next_index[2]\\\\n\\\n",
        "       import pandas as pd\\\\n\\\n",
        "       valid_true_labels = self.inter_feat[self.iid_field][valid_indices].cpu().numpy()\\\\n\\\n",
        "       test_true_labels = self.inter_feat[self.iid_field][test_indices].cpu().numpy()\\\\n\\\n",
        "       valid_user_ids = self.inter_feat[self.uid_field][valid_indices].cpu().numpy()\\\\n\\\n",
        "       test_user_ids = self.inter_feat[self.uid_field][test_indices].cpu().numpy()\\\\n\\\n",
        "       valid_path = os.path.join(self.config[\"output_dir\"], \"valid_true_labels.csv\")\\\\n\\\n",
        "       test_path = os.path.join(self.config[\"output_dir\"], \"test_true_labels.csv\")\\\\n\\\n",
        "       pd.DataFrame({\"user_id\": valid_user_ids, \"true_label\": valid_true_labels}).to_csv(valid_path, index=False)\\\\n\\\n",
        "       pd.DataFrame({\"user_id\": test_user_ids, \"true_label\": test_true_labels}).to_csv(test_path, index=False)' /usr/local/lib/python3.11/dist-packages/recbole/data/dataset/dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3TmTRCmikXw"
      },
      "outputs": [],
      "source": [
        "# change the evaluate signature so we can tell the difference between validation and test sets\n",
        "\n",
        "!sed -i \"s/self, eval_data, load_best_model=True, model_file=None, show_progress=False/self, eval_data, load_best_model=True, model_file=None, show_progress=False, test=False/\" /usr/local/lib/python3.11/dist-packages/recbole/trainer/trainer.py\n",
        "\n",
        "# add parameter of test=True to evaluate method call for the test set\n",
        "!sed -i 's/test_data, load_best_model=saved, show_progress=config\\[\"show_progress\"\\]/test_data, load_best_model=saved, show_progress=config\\[\"show_progress\"\\], test=True/' /usr/local/lib/python3.11/dist-packages/recbole/quick_start/quick_start.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLaPSZ2Oc4KE"
      },
      "outputs": [],
      "source": [
        "# insert code to read user ids and recommendation lists from the new metric and save to csv\n",
        "\n",
        "!sed -i $'/struct = self\\.eval_collector\\.get_data_struct()/a\\\\\n",
        "       import os\\\\n\\\n",
        "       import pandas as pd\\\\n\\\n",
        "       dummy_metric = self.evaluator.metric_class[\"recitems\"]\\\\n\\\n",
        "       dummy_output = dummy_metric.calculate_metric(struct)\\\\n\\\n",
        "       if \"recitems@20\" in dummy_output and \"user_id\" in dummy_output:\\\\n\\\n",
        "           df = pd.DataFrame({\\\\n\\\n",
        "               \"user_id\": dummy_output[\"user_id\"],\\\\n\\\n",
        "               \"predicted_top20\": dummy_output[\"recitems@20\"],\\\\n\\\n",
        "           })\\\\n\\\n",
        "           if test:\\\\n\\\n",
        "               csv_filename = \"linrec_test_predictions.csv\"\\\\n\\\n",
        "           else:\\\\n\\\n",
        "               csv_filename = \"linrec_valid_predictions.csv\"\\\\n\\\n",
        "           csv_path = os.path.join(self.config[\"output_dir\"], csv_filename)\\\\n\\\n",
        "           df.to_csv(csv_path, index=False)\\\\n\\\n",
        "           print(\"CSV saved to\", csv_path)' /usr/local/lib/python3.11/dist-packages/recbole/trainer/trainer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPOH5O0d_ofW"
      },
      "source": [
        "#### ML-1M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjeg4qbR_jMd"
      },
      "outputs": [],
      "source": [
        "# change configurations to run on ML-1M dataset\n",
        "\n",
        "!sed -E -i \"s/^([[:space:]]*)torch\\.distributed\\.barrier\\(\\)/\\1if torch.distributed.is_available() and torch.distributed.is_initialized():\\n\\1    torch.distributed.barrier()/\" /usr/local/lib/python3.11/dist-packages/recbole/data/dataset/dataset.py\n",
        "!sed -i 's/^# from recbole.quick_start.quick_start import run_recbole/from recbole.quick_start.quick_start import run_recbole/' /content/LinRec/run.py\n",
        "!sed -i 's/repeatable: False/repeatable: True/' /content/LinRec/overall.yaml\n",
        "!sed -i \"/^checkpoint_dir: 'saved'/a output_dir: 'output'\" /content/LinRec/overall.yaml\n",
        "!sed -i \"s/run_recbole(model='SASRec', dataset='ML-1M', config_dict=parameter_dict)/run_recbole(model='SASRec', dataset='ml-1m', config_file_list=['ml-1m.yaml', 'overall.yaml'], config_dict=parameter_dict)/\" /content/LinRec/run.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7sGqrAAF_tqH",
        "outputId": "a2109759-ffda-40fd-a7d3-8ca2537ab831"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/usr/local/lib/python3.11/dist-packages/recbole/properties/ml-1m.yaml'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_dir_layers = '/usr/local/lib/python3.11/dist-packages/recbole/model/layers.py'\n",
        "target_dir_yaml = '/usr/local/lib/python3.11/dist-packages/recbole/properties/ml-1m.yaml'\n",
        "\n",
        "shutil.copy('/content/LinRec/layers.py', target_dir_layers)\n",
        "shutil.copy('/content/LinRec/ml-1m.yaml', target_dir_yaml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7QrkzCv_v7Z"
      },
      "outputs": [],
      "source": [
        "!cd /content/LinRec/ && python run.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozTv2zKh_k_g",
        "outputId": "87cf6e41-eb13-49ed-c214-22e8196772d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHckU_1Q_9JG",
        "outputId": "28687ef3-5b4a-49d8-d354-b07dc7165528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined CSV saved to /content/valid_predictions_LinRec_ML1M.csv\n",
            "Combined CSV saved to /content/test_predictions_LinRec_ML1M.csv\n"
          ]
        }
      ],
      "source": [
        "# combine recommendations and labels files into one\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "pred_valid_file = \"/content/drive/MyDrive/linrec_valid_predictions.csv\"\n",
        "label_valid_file = \"/content/drive/MyDrive/valid_true_labels.csv\"\n",
        "output_valid_file = \"valid_predictions_LinRec_ML1M.csv\"\n",
        "\n",
        "pred_test_file = \"/content/drive/MyDrive/linrec_test_predictions.csv\"\n",
        "label_test_file = \"/content/drive/MyDrive/test_true_labels.csv\"\n",
        "output_test_file = \"test_predictions_LinRec_ML1M.csv\"\n",
        "\n",
        "# create combined validation predictions file\n",
        "pred_valid_df = pd.read_csv(pred_valid_file)\n",
        "pred_valid_df['user_id'] = pred_valid_df['user_id'] + 1\n",
        "\n",
        "label_valid_df = pd.read_csv(label_valid_file)\n",
        "combined_df = pd.merge(pred_valid_df, label_valid_df, on=\"user_id\", how=\"left\")\n",
        "\n",
        "combined_df.to_csv(output_valid_file, index=False)\n",
        "print(\"Combined CSV saved to\", os.path.abspath(output_valid_file))\n",
        "\n",
        "# create combined test predictions file\n",
        "pred_test_df = pd.read_csv(pred_test_file)\n",
        "pred_test_df['user_id'] = pred_test_df['user_id'] + 1\n",
        "\n",
        "label_test_df = pd.read_csv(label_test_file)\n",
        "combined_df = pd.merge(pred_test_df, label_test_df, on=\"user_id\", how=\"left\")\n",
        "\n",
        "combined_df.to_csv(output_test_file, index=False)\n",
        "print(\"Combined CSV saved to\", os.path.abspath(output_test_file))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZhrpbzYEhUA"
      },
      "source": [
        "#### Amazon Beauty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVjWXRJ0Et_B"
      },
      "outputs": [],
      "source": [
        "# change configuration to run on Amazon Beauty dataset\n",
        "\n",
        "!sed -E -i \"s/^([[:space:]]*)torch\\.distributed\\.barrier\\(\\)/\\1if torch.distributed.is_available() and torch.distributed.is_initialized():\\n\\1    torch.distributed.barrier()/\" /usr/local/lib/python3.11/dist-packages/recbole/data/dataset/dataset.py\n",
        "!sed -i 's/^# from recbole.quick_start.quick_start import run_recbole/from recbole.quick_start.quick_start import run_recbole/' /content/LinRec/run.py\n",
        "!sed -i \"s/'train_batch_size': 2048/'train_batch_size': 1024/\" /content/LinRec/run.py\n",
        "!sed -i \"s/'eval_batch_size': 2048/'eval_batch_size': 1024/\" /content/LinRec/run.py\n",
        "!sed -i \"s/'hidden_size': 128/'hidden_size': 64/\" /content/LinRec/run.py\n",
        "!sed -i 's/repeatable: False/repeatable: True/' /content/LinRec/overall.yaml\n",
        "!sed -i \"/^checkpoint_dir: 'saved'/a output_dir: 'output'\" /content/LinRec/overall.yaml\n",
        "!sed -i \"s/run_recbole(model='SASRec', dataset='ML-1M', config_dict=parameter_dict)/run_recbole(model='SASRec', dataset='amazon-beauty', config_file_list=['amazon-beauty.yaml', 'overall.yaml'], config_dict=parameter_dict)/\" /content/LinRec/run.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXHC-WEHE2-c",
        "outputId": "7f690e28-6c5d-4254-82b1-6c2bb97cd1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YAML file created at: /content/LinRec/amazon-beauty.yaml\n"
          ]
        }
      ],
      "source": [
        "beauty_yaml_file_path = \"/content/LinRec/amazon-beauty.yaml\"\n",
        "\n",
        "yaml_content = \"\"\"\n",
        "# Atomic File Format\n",
        "field_separator: \"\\t\"           # (str) Separator of different columns in atomic files.\n",
        "seq_separator: \" \"              # (str) Separator inside the sequence features.\n",
        "\n",
        "# Basic Information\n",
        "USER_ID_FIELD: user_id          # (str) Field name of user ID feature.\n",
        "ITEM_ID_FIELD: item_id          # (str) Field name of item ID feature.\n",
        "RATING_FIELD: rating            # (str) Field name of rating feature.\n",
        "TIME_FIELD: timestamp           # (str) Field name of timestamp feature.\n",
        "seq_len: ~                      # (dict) Field name of sequence feature: maximum length of each sequence\n",
        "LABEL_FIELD: label              # (str) Expected field name of the generated labels for point-wise dataLoaders.\n",
        "threshold: ~                    # (dict) 0/1 labels will be generated according to the pairs.\n",
        "NEG_PREFIX: neg_                # (str) Negative sampling prefix for pair-wise dataLoaders.\n",
        "\n",
        "# Sequential Model Needed\n",
        "ITEM_LIST_LENGTH_FIELD: item_length   # (str) Field name of the feature representing item sequences' length.\n",
        "LIST_SUFFIX: _list              # (str) Suffix of field names which are generated as sequences.\n",
        "MAX_ITEM_LIST_LENGTH: 200        # (int) Maximum length of each generated sequence.\n",
        "POSITION_FIELD: position_id     # (str) Field name of the generated position sequence.\n",
        "\n",
        "# Knowledge-based Model Needed\n",
        "HEAD_ENTITY_ID_FIELD: head_id   # (str) Field name of the head entity ID feature.\n",
        "TAIL_ENTITY_ID_FIELD: tail_id   # (str) Field name of the tail entity ID feature.\n",
        "RELATION_ID_FIELD: relation_id  # (str) Field name of the relation ID feature.\n",
        "ENTITY_ID_FIELD: entity_id      # (str) Field name of the entity ID.\n",
        "kg_reverse_r: False             # (bool) Whether to reverse relations of triples for bidirectional edges.\n",
        "entity_kg_num_interval: ~       # (str) Entity interval for filtering kg, such as [A,B] / [A,B) / (A,B) / (A,B].\n",
        "relation_kg_num_interval: ~     # (str) Relation interval for filtering kg, such as [A,B] / [A,B) / (A,B) / (A,B].\n",
        "\n",
        "# Selectively Loading\n",
        "load_col:                       # (dict) The suffix of atomic files: (list) field names to be loaded.\n",
        "    inter: [user_id, item_id, rating, timestamp]\n",
        "unload_col: ~                   # (dict) The suffix of atomic files: (list) field names NOT to be loaded.\n",
        "unused_col: ~                   # (dict) The suffix of atomic files: (list) field names which are loaded but not used.\n",
        "\n",
        "# Filtering\n",
        "rm_dup_inter: ~                 # (str) Whether to remove duplicated user-item interactions.\n",
        "val_interval: ~                 # (dict) Filter inter by values in {value field (str): interval (str)}.\n",
        "filter_inter_by_user_or_item: True    # (bool) Whether or not to filter inter by user or item.\n",
        "user_inter_num_interval: \"[5, inf)\"     # (str) User interval for filtering inter, such as [A,B] / [A,B) / (A,B) / (A,B].\n",
        "item_inter_num_interval: \"[5, inf)\"     # (str) Item interval for filtering inter, such as [A,B] / [A,B) / (A,B) / (A,B].\n",
        "\n",
        "# Preprocessing\n",
        "alias_of_user_id: ~             # (list) Fields' names remapped into the same index system with USER_ID_FIELD.\n",
        "alias_of_item_id: ~             # (list) Fields' names remapped into the same index system with ITEM_ID_FIELD.\n",
        "alias_of_entity_id: ~           # (list) Fields' names remapped into the same index system with ENTITY_ID_FIELD.\n",
        "alias_of_relation_id: ~         # (list) Fields' names remapped into the same index system with RELATION_ID_FIELD.\n",
        "preload_weight: ~               # (dict) Preloaded weight in {IDs (token): pretrained vectors (float-like)}.\n",
        "normalize_field: ~              # (list) List of filed names to be normalized.\n",
        "normalize_all: True            # (bool) Whether or not to normalize all the float like fields.\n",
        "\"\"\"\n",
        "\n",
        "with open(beauty_yaml_file_path, \"w\") as file:\n",
        "    file.write(yaml_content)\n",
        "\n",
        "print(f\"YAML file created at: {beauty_yaml_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z0pMuhHHE9nV",
        "outputId": "441eafc3-0f0c-485f-e9ce-21a49a26aafe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/usr/local/lib/python3.11/dist-packages/recbole/properties/amazon-beauty.yaml'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_dir_layers = '/usr/local/lib/python3.11/dist-packages/recbole/model/layers.py'\n",
        "target_dir_yaml = '/usr/local/lib/python3.11/dist-packages/recbole/properties/amazon-beauty.yaml'\n",
        "\n",
        "shutil.copy('/content/LinRec/layers.py', target_dir_layers)\n",
        "shutil.copy('/content/LinRec/amazon-beauty.yaml', target_dir_yaml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z5g10qdE_Wx"
      },
      "outputs": [],
      "source": [
        "!cd /content/LinRec/ && python run.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "o8SPrhonFDzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b490d4e6-286c-48f1-d14e-dfb54dd627ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kbIpf2kbFHRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab21e86b-509a-4a60-9b14-27c481d7e864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined CSV saved to /content/valid_predictions_LinRec_Beauty.csv\n",
            "Combined CSV saved to /content/test_predictions_LinRec_Beauty.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "pred_valid_file = \"/content/drive/MyDrive/linrec_valid_predictions_beauty.csv\"\n",
        "label_valid_file = \"/content/drive/MyDrive/valid_true_labels_beauty.csv\"\n",
        "output_valid_file = \"valid_predictions_LinRec_Beauty.csv\"\n",
        "\n",
        "pred_test_file = \"/content/drive/MyDrive/linrec_test_predictions_beauty.csv\"\n",
        "label_test_file = \"/content/drive/MyDrive/test_true_labels_beauty.csv\"\n",
        "output_test_file = \"test_predictions_LinRec_Beauty.csv\"\n",
        "\n",
        "# create combined validation predictions file\n",
        "pred_valid_df = pd.read_csv(pred_valid_file)\n",
        "pred_valid_df['user_id'] = pred_valid_df['user_id'] + 1\n",
        "\n",
        "label_valid_df = pd.read_csv(label_valid_file)\n",
        "combined_df = pd.merge(pred_valid_df, label_valid_df, on=\"user_id\", how=\"left\")\n",
        "\n",
        "combined_df.to_csv(output_valid_file, index=False)\n",
        "print(\"Combined CSV saved to\", os.path.abspath(output_valid_file))\n",
        "\n",
        "# create combined test predictions file\n",
        "pred_test_df = pd.read_csv(pred_test_file)\n",
        "pred_test_df['user_id'] = pred_test_df['user_id'] + 1\n",
        "\n",
        "label_test_df = pd.read_csv(label_test_file)\n",
        "combined_df = pd.merge(pred_test_df, label_test_df, on=\"user_id\", how=\"left\")\n",
        "\n",
        "combined_df.to_csv(output_test_file, index=False)\n",
        "print(\"Combined CSV saved to\", os.path.abspath(output_test_file))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "E3Jj5RBdVdAb",
        "ifHFr2bW_ZxD"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}